class DefaultConfigs:
    config_path = 'config.py'

    ############################################
    # modify this part to set up the experiments
    lg1 = 'en'
    lg2 = 'fr' # or 'de', 'es', 'it'
    # train_data_path = "/mnt/hinoki/mao/data/ParaCrawl/test" # find this in the "data.zip" file
    # train_data_path = "/mnt/elm/mao/data/paracrawl/paracrawl.enfr.shuf.lower.clean"
    # train_data_path = "/mnt/elm/mao/data/paracrawl/test.5M"
    # train_data_path = "raw_data/32languages/32languages.4/32languages.train.3"
    # train_data_path = "tatoeba_data/en-fr/train.en-fr"
    train_data_path = "/mnt/osmanthus/mao/lightweight/raw_data/data/62languages.uncased.train.abci.7.shuf"
    # due to size limitation, we only uploaded a test file of English-French for your checking
    # valid_data_path = "tatoeba_data/en-fr/valid.en-fr"
    valid_data_path = "/mnt/osmanthus/mao/lightweight/raw_data/data/62languages.uncased.valid"
    # valid_data_path = "raw_data/32languages/32languages.0.valid"
    validation_size = 50219
    # validation_size = 50 # for the training described in the paper, we used 10000 for validation. Here we use 100 sentences for test.
    ############################################
    
    step_checkpoint = True
    epoch_checkpoint = False
    num_checkpoint_per_epoch = 6
    output_dir = "ckpt"
    best_models =  "best_model/"
    pooling_method = 'MEAN'
    ffn_act = 'swish'
    hiddent_act = 'swish'
    bpe_path = "preprocessing/62languages_bpe_60000.model"
    weights_path = "raw_data/stat.txt"
    lower = True
    do_oversampling = False
    oversampling_T = 5
    num_sample = 40800000
    # num_sample = 500000
    vocab_size = 60000
    la_num = 62
    ddp_sampler = False
    has_la_emb = True
    share_emb = False
    generative_task = "UGT"
    
    # hyperparameters
    max_seq_len = 120 # 120 256 512
    train_batch_size = 168
    vali_batch_size = 64
    num_layers = 2
    sent_dim = 512
    token_dim = 512
    la_dim = 128
    num_heads = 8
    ffn_dim = 1024
    neg_samples = 4
    mlmrate = 0.15
    contrastive_T = 0.1
    contrastive_method = "cos"
    sentence_alignment_loss_weight = 2
    sentence_similarity_loss_weight = 2
    weight_decay = 1e-5
    learning_rate = 1e-3
    num_train_epochs = 15
    milestones = [epoch + 1 for epoch in range(num_train_epochs-1)]
    has_validation = True
    
    # sentence evaluation part
    eval_data_path = 'data/'
    eval_batch_size = 256
    n_keys = 200000
    n_queries = 2000
    method = 'nn'        
   
    def set_config(self, model_name, resume=False, is_train=True):
        if is_train:
            self.emb_dropout = 0.1
            self.attention_dropout = 0.1
            self.hidden_dropout = 0.1
            self.sent_dropout = 0.0
            self.xtr_dropout = 0.0
        else:
            self.emb_dropout = 0.0
            self.attention_dropout = 0.0
            self.hidden_dropout = 0.0
            self.bpe_path = '../preprocessing/62languages_bpe_60000.model'
            self.output_dir = "../" + output_dir
        self.resume = resume
        self.model_name = model_name
        
        # experimental settings for UGT and UGT+ALIGN+SIM
        # other settings can be added here either for hyperparameter search and ablation study
        if model_name == 'maki19':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 160
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.has_la_emb = True
        elif model_name == 'maki20':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 160
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.has_la_emb = True
        elif model_name == 'maki21':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki22':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 168
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.has_la_emb = True
            self.share_emb = True
        elif model_name == 'maki23':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki24':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 100
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.has_la_emb = True
            self.share_emb = False
            self.ffn_act = 'gelu_new'
            self.hiddent_act = 'gelu_new'
        elif model_name == 'maki25':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki26':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki27':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 0.01
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki28':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 1.0
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki29':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 0.05
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki30':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 0.5
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki31':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 0.2
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki32':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 0.3
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki33':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 0.08
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki34':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.contrastive_T = 0.09
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki35':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 1.0
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki36':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki37':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "NOXTR"
            self.contrastive_T = 1.0
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki38':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 1.0
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki39':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.5
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki40':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 1.0
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki41':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki42':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.sentence_alignment_loss_weight = 10
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki43':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 1e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki44':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki45':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-4
        elif model_name == 'maki46':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 0.0
        elif model_name == 'maki47':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-6
        elif model_name == 'maki48':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 5e-6
        elif model_name == 'maki49':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
        elif model_name == 'maki50':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.has_la_emb = True
            self.share_emb = False
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.1
        elif model_name == 'maki51': # max_length = 256
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 64
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.contrastive_T = 0.1
            self.sent_dropout = 0.0
            self.has_la_emb = True
            self.share_emb = False
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.1
        elif model_name == 'maki52':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.3
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.3
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki53':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.0
                self.attention_dropout = 0.0
                self.hidden_dropout = 0.0
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki54':
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = True
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki55': # no la_emb
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = False
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki56': # only XTR
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = False
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki57': # maki49 on A100
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 200
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki58': # only align
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki59': # UGT+align
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "UGT"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki60': # no linear
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki61': # share emb
            self.num_layers = 6
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 152
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = True
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki62': # 2 layer
            self.num_layers = 2
            self.sent_dim = 1024
            self.token_dim = 1024
            self.num_heads = 16
            self.ffn_dim = 4096
            self.train_batch_size = 384
            self.vali_batch_size = 32
            self.learning_rate = 3e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.generative_task = "XTR"
            self.has_la_emb = True
            self.share_emb = False
            self.weight_decay = 1e-5
            self.contrastive_T = 0.1
            if is_train:
                self.emb_dropout = 0.1
                self.attention_dropout = 0.1
                self.hidden_dropout = 0.1
                self.sent_dropout = 0.0
                self.xtr_dropout = 0.0
        elif model_name == 'maki_debug':
            self.num_layers = 2
            self.sent_dim = 512
            self.token_dim = 512
            self.num_heads = 8
            self.ffn_dim = 1024
            self.train_batch_size = 32
            self.vali_batch_size = 32
            self.learning_rate = 5e-4
            self.has_sentence_loss = True
            self.has_sentence_similarity_loss = False
            self.has_la_emb = True
            self.share_emb = False
        else:
            print('No such experiments! Please check.')
            raise

config = DefaultConfigs()

